{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Versuch Recommender Systeme\n",
    "\n",
    "* Prof: Dr. Johannes Maucher\n",
    "* Project: Janina Mattes\n",
    "* Datum: 22.05.2020\n",
    "\n",
    "[Übersicht Ipython Notebooks im Data Mining Praktikum](Data Mining Praktikum.ipynb)\n",
    "\n",
    "\n",
    "# Einführung\n",
    "## Lernziele:\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* __Ähnlichkeit:__ Verfahren zur Bestimmung der Ähnlichkeit zwischen Personen (Kunden) und Elementen (Produkten)\n",
    "* __Empfehlungssysteme__ Collaborative Filtering \n",
    "* __Collaborative Filtering:__ Nutzerbezogener Ansatz und elementbasierter Ansatz\n",
    "\n",
    "Sämtliche Verfahren und Algorithmen werden in Python implementiert.\n",
    "\n",
    "## Theorie zur Vorbereitung\n",
    "### Recommender Systeme\n",
    "Recommender Systeme werden im E-Commerce eingesetzt um Werbung in Form von kundenspezifischen Empfehlungen zu verteilen. Weitläufig bekannt sind die Amazon-Empfehlungen, die entweder per e-mail geschickt oder nach dem Log-In in der Web-Page angezeigt werden. Diese Empfehlungen werden in Abhängigkeit von den bisher vom jeweiligen Kunden gekauften bzw. bewerteten Produkten erstellt. In diesem Versuch werden die derzeit wohl am weitest verbreiteteten Verfahren für die Erzeugung kundenspezifischer Empfehlungen vorgestellt, darunter das elementweise Collaborative Filtering, welches z.B. auch von Amazon eingesetzt wird.     \n",
    "\n",
    "Direkt-Marketing Methoden wie die kundenspezifische Erzeugung und Bereitstellung von Werbung erfordern detaillierte Kunden- und Warenkorbanalysen. Kunden mit ähnlichem Kaufverhalten werden in Kundengruppen zusammengefasst. Die Warenkorbanalyse untersucht u.a. welche Waren bevorzugt im Verbund von der gleichen Person gekauft werden. Damit kann ein Händler Werbung in Form von Empfehlungen individuell und gezielt an seine Kunden richten, abhängig davon welcher Kundengruppe er angehört und welche Produkte bevorzugt von dieser Kundengruppe nachgefragt werden. \n",
    "\n",
    "Im ersten Teil der Übung werden fiktive Daten in einer überschaubaren Menge verwendet. Es handelt sich hier um Filmbewertungen. Anhand dieses Beispiels sollen die notwendigen Methoden und Abläufe implementiert und getestet werden. Diese werden im zweiten Teil der Übung auf echte Daten angewandt. Hierzu werden über eine Python-API Daten vom Internet-Meta-Radio _last.fm_ integriert. Auf der Basis dieser Daten sollen dann Musikempfehlungen für last.fm User berechnet werden. \n",
    "\n",
    "Recommender Systeme lassen sich mit\n",
    "\n",
    "* Clustering Verfahren\n",
    "* Suchalgorithmen\n",
    "* Collaborativen Filtering \n",
    " \n",
    "realisieren. Am häufigsten wird hierbei das Collaborative Filtering eingesetzt. Für das Collaborative Filtering wird jeder der $M$ User durch einen $N$-dimensionalen Vektor beschrieben, wobei $N$ die Anzahl der Produkte im Angebot des Händlers ist. Jedes Element im Vektor gehört zu einem speziellen Produkt. Das Element hat den Wert 1, wenn der User dieses Produkt bereits gekauft hat, sonst 0 (andere Wertbelegungen sind möglich, z.B. wenn Produktbewertungen vorliegen). Alle $M$ Zeilenvektoren können zur _User/Item_ Matrix zusammengefasst werden (siehe Abbildung).\n",
    "\n",
    "![Abbildung User Item Matrix](https://www.hdm-stuttgart.de/~maucher/ipnotebooks/DataMining//Bilder/UserItemMatrix.png \"User Item Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use this cell if you work from Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das traditionelle **userbasierte Collaborative Filtering (UCF)**, benutzt die Ähnlichkeit zwischen Benutzern: Um für User $U_i$ eine Empfehlung zu erzeugen wird zunächst der diesem User ähnlichste Kunde (oder eine Menge vom ähnlichsten Kunden) ermittelt. Dann werden $U_i$ die Produkte (Items) empfohlen, welche der ähnlichste Kunde gekauft hat, $U_i$ selbst jedoch noch nicht. \n",
    "\n",
    "Dieser Ansatz skaliert schlecht im Fall sehr großer _User/Item_-Matrizen. Ausserdem ist er für User, welche erst wenige Produkte gekauft haben unzuverlässig. Besser eignet sich in diesen Fällen das **itembasierte Collaborative Filtering (ICF)**. Es wird u.a. von Amazon.com eingesetzt. Diese Variante benutzt die Ähnlichkeit zwischen Produkten (Items). Dabei sind Produkte umso ähnlicher je mehr Kunden diese Produkte gemeinsam gekauft haben. Für die Produkte welche ein Referenzuser $U_i$ bereits gekauft hat, werden die ähnlichsten Produkte ermittelt. Diese ähnlichsten Produkte werden $U_i$ empfohlen, wenn er sie nicht schon selbst gekauft hat.\n",
    "\n",
    "Im folgenden Abschnitt werden einige gebräuchliche Metriken für die Berechnung der Ähnlichkeit zwischen Benutzern oder Artikeln vorgestellt. Für Collaboratives Filtering wird sehr häufig das Cosinus - Ähnlichkeitsmaß eingesetzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebräuchliche Ähnlichkeitsmaße\n",
    "\n",
    "Die __euklidische Distanz__ $d_E(\\underline{a},\\underline{b})$ zwischen zwei n-dimensionalen Vektoren $\\underline{a}=(a_1,\\ldots,a_n)$ und $\\underline{b}=(b_1,\\ldots,b_n)$ berechnet sich zu\n",
    "\t$$\n",
    "\td_E(\\underline{a},\\underline{b})=\\sqrt{\\sum_{i=1}^n (a_i-b_i)^2}\n",
    "\t$$\n",
    "Zwei Vektoren können als umso ähnlicher erachtet werden, je kleiner deren euklidische Distanz ist. \n",
    "Ein auf der euklidischen Metrik basierendes Ähnlichkeitsmaß zwischen zwei Vektoren $\\underline{a}$ und $\\underline{b}$ kann durch \n",
    "$$\n",
    "s_E(\\underline{a},\\underline{b})=\\frac{1}{1+d_E(\\underline{a},\\underline{b})}\n",
    "$$\n",
    "angegeben werden.\n",
    "\n",
    "\n",
    "__Pearson Korrelation__\n",
    "Die Ähnlichkeit zwischen zwei Vektoren kann auch durch den Pearson-Korrelationskoeffizient $\\rho_{\\underline{a},\\underline{b}}$ ausgedrückt werden. Er berechnet sich zu\n",
    "$$\n",
    "\\rho_{\\underline{a},\\underline{b}}= \\frac{1}{N}\\cdot \\sum\\limits_{i=1}^{N}\\frac{(a_i-\\overline{a})}{\\sigma_a} \\frac{(b_i-\\overline{b})}{\\sigma_b}\n",
    "$$\n",
    "Dabei bezeichnet $N$ die Länge der Vektoren, $\\overline{a}$ den Mittelwert und $\\sigma_a$ die Standardabweichung des Vektors $\\underline{a}$. \n",
    "\n",
    "Der Pearson-Korrelationskoeffizient misst die lineare Abhängigkeit zwischen zwei Vektoren. Der maximale Wert von $+1$ wird erreicht, wenn die durch die beiden Vektoren definierten N Punkte im 2-dimensionalen Raum auf einer ansteigenden Geraden liegen. Der Minimalwert von $-1$ wird erreicht, wenn die Punkte auf einer abfallenden Geraden liegen. Der Betrag des Koeffizienten ist umso kleiner, je stärker die Punkte von einer fiktiven Geraden (kann durch lineare Regression berechnet werden) abweichen. Der Koeffizient ist $0$ wenn keine lineare Abhängigkeit zwischen den Vektoren besteht.\n",
    "\n",
    "\n",
    "__Cosinus Ähnlichkeitsmaß__\n",
    "Die Ähnlichkeit zwischen zwei Vektoren kann auch durch den Cosinus $\\cos(\\underline{a},\\underline{b})$ ausgedrückt werden. Er berechnet sich zu\n",
    "$$\n",
    "\\cos(\\underline{a},\\underline{b})= \\frac{\\underline{a} \\cdot \\underline{b}}{\\left\\|\\underline{a}\\right\\|\\cdot \\left\\|\\underline{b}\\right\\|}\n",
    "$$\n",
    "wobei im Zähler das Skalarprodukt der beiden Vektoren steht und mit $\\left\\|\\underline{x}\\right\\|$ der Betrag des Vektors $\\underline{x}$ bezeichnet wird.\n",
    "\n",
    "Falls die Vektoren $\\underline{a}$ und $\\underline{b}$ mittelwertfrei sind, ist der Cosinus-Ähnlichkeitswert gleich dem Pearson-Korrelationswert. In der Dokument- und Textanalyse wird vornehmlich das Cosinus-Ähnlichkeitsmaß verwendet. \n",
    "\n",
    "\n",
    "__Russel Rao Ähnlichkeitsmaß__\n",
    "Die Russel Rao-Ähnlichkeit zwischen zwei binären Vektoren $\\underline{a}$ und $\\underline{b}$ mißt das Verhältnis zwischen der Anzahl $\\alpha$ der Stellen in denen beide Vektoren den Wert 1 haben und der Länge $n$ der Vektoren. Z.B. ist für die Vektoren $\\underline{a}=(1,0,1,0,0,1)$ und $\\underline{b}=(0,1,1,1,0,1)$ die Russel-Rao-Ähnlichkeit $s_{RR}(\\underline{a},\\underline{b})=2/6=0.333$.\n",
    "\n",
    "__Jaccard Ähnlichkeitsmaß__\n",
    "Die Jaccard-Ähnlichkeit zwischen zwei binären Vektoren $\\underline{a}$ und $\\underline{b}$ mißt das Verhältnis zwischen der Anzahl $\\alpha$ der Stellen in denen beide Vektoren den Wert $1$ haben und der Anzahl der Stellen in denen mindestens einer der beiden Vektoren ungleich $0$ ist. Z.B. ist für die Vektoren $\\underline{a}=(1,0,1,0,0,1)$ und $\\underline{b}=(0,1,1,1,0,1)$ die Jaccard-Ähnlichkeit $s_{J}(\\underline{a},\\underline{b})=2/5=0.4$. Die Jaccard Metrik wird in diesem Versuch für die Bestimmung der Ähnlichkeit von _last.fm_-Usern eingesetzt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vor dem Versuch zu klärende Fragen\n",
    "Eine Untermenge der im Folgenden aufgeführten Fragen wird zu Beginn des Versuchs im Rahmen eines Gruppenkolloqs abgefragt. Auf jede Frage sollte von mindestens einem Gruppenmitglied eine Antwort geliefert werden und jedes Gruppenmitglied muss mindestens eine der gestellten Fragen beantworten können.\n",
    "\n",
    "**Aufgaben:**\n",
    "\n",
    "* Beschreiben Sie das Prinzip des userbasierten Collaborativen Filtering (UCF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Prinzip des _userbasierten Collaborativen Filtering (UCF)_ betrachtet die Ähnlichkeit zwischen zwei Nutzern/Kunden mittels verschiedener Ähnlichkeitsmaßen zum Beispiel der euklidische Distanz oder Pearson Korrelation. Dies bedeutet, um einem Kunden $U_i$ eine Produktempfehlung zu machen wird zuerst eine Menge, diesem Kunden $U_i$ ähnlichen, anderen Kunden ermittelt. Auf Basis dieser kann dem Kunden $U_i$ anschließend eine Produktempfehlung gemacht werden für Produkte, die dieser selbst noch nicht gekauft hat, welche jedoch bereits durch den am ähnlichsten Kunden gekauft wurden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Welche Nachteile hat das UCF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Schwierigkeit des _UCF_ Verfahrens ist, dass dies im Falle sehr großer _User/Item_-Matrizen nur schlecht skaliert. Darüberhinaus ist die Anwendung dieses Prinzips nicht zuverlässig bei Nutzern, welche erst wenige Produkte gekauft haben. \n",
    "\n",
    "In der Praxis übertrifft ein Item-Item-Modell in vielen Anwendungsfällen ein Benutzer-Benutzer-Modell, da man sagen kann, dass Produkte (Items) \"einfacher\" sind als Benutzer. Das bedeutet, dass Produkte (Items) zu einer kleinen Gruppe von Genres gehören, während die Benutzer zum Beispiel sehr unterschiedliche Geschmäcker haben. Außerdem ist die Ähnlichkeit von Items aussagekräftiger als die Ähnlichkeit von Benutzern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Worin besteht der Unterschied zwischen UCF und itembasierten Collaborativen Filtering (ICF)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das _itembasierte Collaborative Filtering (ICF)_ basiert im Gegensatz zum _UCF_ Prinzip auf der Ähnlichkeit zwischen Produkten (Items). Je mehr Kunden bestimmte Produkte gemeinsam gekauft haben, desto ähnlicher sind diese. Auf dieser Basis werden die ähnlichsten Produkte ermittelt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gegeben seien die Vektoren \n",
    "\n",
    "    \\begin{eqnarray*}\n",
    "    \\underline{a} & = & [1,2,3,4,5,6] \\\\\n",
    "    \\underline{b} & = & [3,3,5,6,7,8] \\\\\n",
    "    \\end{eqnarray*}\n",
    "    \n",
    "    Schreiben Sie eine Python Funktion, die den Mittelwert derartiger Vektoren berechnet. Schreiben Sie eine weitere Funktion, die die Varianz berechnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mittelwert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectors\n",
    "\n",
    "a=[1,2,3,4,5,6]\n",
    "b=[3,3,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of vector a:3.5\n",
      "Mean of vector b:5.333333333333333\n"
     ]
    }
   ],
   "source": [
    "# 1. Aufgabe Mean Value\n",
    "\n",
    "def mean(vector):\n",
    "    sum = 0\n",
    "    for i in vector:\n",
    "        sum += i\n",
    "    return sum/len(vector)\n",
    "\n",
    "print(\"Mean of vector a:{}\".format(mean(a)))\n",
    "print(\"Mean of vector b:{}\".format(mean(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varianz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of vector a:2.9166666666666665\n",
      "Variance of vector b:3.5555555555555554\n"
     ]
    }
   ],
   "source": [
    "# 2. Aufgabe Varianz and Deviation\n",
    "\n",
    "def variance(vector, mean):\n",
    "    sum = 0\n",
    "    for i in vector:\n",
    "        sum += abs(mean - i) ** 2\n",
    "    return sum/len(vector)\n",
    "\n",
    "print(\"Variance of vector a:{}\".format(variance(a, mean(a))))\n",
    "print(\"Variance of vector b:{}\".format(variance(b, mean(b))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie groß ist die\n",
    "\n",
    "    - Euklidische Ähnlichkeit\n",
    "    - Pearson Ähnlichkeit\n",
    "    - Cosinus Ähnlichkeit\n",
    "    \n",
    "    zwischen den Vektoren $\\underline{a}$ und $\\underline{b}$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aufgabe \n",
    "# Euklidische, Pearson und Cosinus Ähnlichkeit\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "from scipy import spatial, stats # sonst evtl OS Probleme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Euklidische Ähnlichkeit\n",
    "\n",
    "Zwei Vektoren können als umso ähnlicher erachtet werden, je kleiner deren euklidische Distanz ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euklidean Distance: 4.58257569495584\n"
     ]
    }
   ],
   "source": [
    "print(\"Euklidean Distance: {}\".format(scipy.spatial.distance.euclidean(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pearson Ähnlichkeit\n",
    "\n",
    "Der Pearson-Korrelationskoeffizient misst die lineare Abhängigkeit zwischen zwei Vektoren. Der maximale Wert ist +1 und der Minimalwert -1. Der Betrag des Koeffizienten ist umso kleiner, je stärker die Punkte von einer fiktiven Geraden (kann durch lineare Regression berechnet werden) abweichen. Der Koeffizient ist  0  wenn keine lineare Abhängigkeit zwischen den Vektoren besteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Distance/Correlation: (0.9833434220628547, 0.0004138517691427288)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson Distance/Correlation: {}\".format(scipy.stats.pearsonr(a,b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosinus Ähnlichkeit\n",
    "\n",
    "Kosinus-Ähnlichkeit ist ein Maß für die Ähnlichkeit zweier Vektoren. Die Kosinus-Ähnlichkeit liegt zwischen −1 (genau entgegengerichtet) und 1 (genau gleichgerichtet). Ein Wert von 0 bedeutet üblicherweise Unabhängigkeit (Orthogonalität). Zwischenwerte zeigen Ähnlichkeit oder Unähnlichkeit an.\n",
    "In der Dokument- und Textanalyse wird vornehmlich das Cosinus-Ähnlichkeitsmaß verwendet.\n",
    "\n",
    "\n",
    "Die Berechnung der Kosinusdistanz mit normalisierten Werten ( _zentrierte Kosinusähnlichkeit_ ) erfasst die Intuition besser, da fehlende Bewertungen als \"Durchschnitt\" behandelt werden und sie kann sowohl mit \"harten Bewertern\" als auch mit \"leichten Bewertern\" einfach umgehen. Auf diese Weise zeigt ein Wert nahe Null eine höhere Ähnlichkeit.\n",
    "Dieses Ähnlichkeitsmaß ist auch bekannt als **Pearson-Korrelation** ( _zentrierte Cosinus-Ähnlichkeit_ ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosinus Similarity: 0.9910600847451639\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosinus Similarity: {}\".format(1-scipy.spatial.distance.cosine(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In welchen Fällen sind Cosinus- und Pearsonähnlichkeit der euklidischen Ähnlichkeit vorzuziehen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die euklidische Ähnlichkeit nutzt absolute Werte zur Berechnung der Ähnlichkeit. Das heißt, wenn Werte skaliert werden ändert sich auch die euklidische Ähnlichkeit.\n",
    "\n",
    "Bei Cosinus- und Pearsonähnlichkeit ändern sich die Werte beim skalieren nicht. Man nutzt diese Ähnlichkeiten wenn man Korrelationen unabhängig von der absoluten Distanz von Werten bestimmen will.\n",
    "\n",
    "Die Kosinusähnlichkeit betrachtet den Winkel zwischen zwei Vektoren, während die euklidische Ähnlichkeit den Abstand zwischen zwei Punkten misst. Falls die Vektoren _a_, _b_ keinen Mittelwert haben, d.h. mittelwertfrei sind, so ist der Kosinus-Ähnlichkeitswert gleich dem Pearson-Korrelationswert. Die Kosinusähnlichkeit wird im Allgemeinen als Metrik zur Entfernungsmessung verwendet, wenn die Größe der Vektoren keine Rolle spielt. \n",
    "Dies geschieht z.B. bei der Arbeit mit Textdaten, die durch Wortzahlen dargestellt werden. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuchsdurchführung\n",
    "## Teil 1: Fiktive Filmbewertung\n",
    "### Daten\n",
    "Folgende Tabelle enthält die Filmbewertungen von 7 Personen.\n",
    "from IPython.display import Latex\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "![Abbildung Bewertung Fiktive Kunden](https://www.hdm-stuttgart.de/~maucher/ipnotebooks/DataMining/Bilder/recommenderFilmRecommendations.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Tabelle ist als Python dictionary _critics_ implementiert. Die Keys des Python-Dictionary definieren die Namen von Personen (Zeilen in der Matrix), die Filme bewertet haben. Die Values sind selbst wieder Dictionarys, welche als Keys die Filmnamen (Spalten in der Matrix) und als Values die jeweilige Filmbewertung (Matrixelment) enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n",
    " 'You, Me and Dupree': 3.5}, \n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0}, \n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ähnlichkeiten berechnen\n",
    "\n",
    "Für die Bestimmung der Ähnlichkeit zwischen Personen und Produkten werden in diesem Versuch ein auf der euklidischen Distanz basierendes Ähnlichkeitsmaß und die Pearson-Korrelation verwendet. Beide Ähnlichkeitsmaße sind in den unten definierten Funktionen implementiert. Alle drei hier implementierten Funktionen zur Berechnung der Ähnlichkeit erhalten als Übergabeparameter das oben definierte Dictionary, das die Filmbewertungen enthält und die Namen der zwei Personen, die verglichen werden sollen. \n",
    "\n",
    "Zu beachten ist, dass in beiden Funktionen für die Berechnung der Ähnlichkeit zwischen zwei Personen nur die Produkte berücksichtigt werden, welche von beiden Personen schon bewertet wurden. Es handelt sich hier also um modifizierte Ähnlichkeitsfunktionen. \n",
    "\n",
    "__Aufgabe:__\n",
    "Fragen Sie von diesem Dictionary _Toby's_ Bewertung des Films _Snakes on a Plane_ ab und geben Sie diesen Wert aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n"
     ]
    }
   ],
   "source": [
    "# Gebe die Bewertung vom Film \"Snakes on a Plane\" von Toby aus\n",
    "print(critics.get(\"Toby\").get(\"Snakes on a Plane\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sci\n",
    "\n",
    "\n",
    "def sim_euclid(prefs, person1, person2, normed=True):\n",
    "  ''' Returns a euclidean-distance-based similarity score for \n",
    "  person1 and person2. In the distance calculation the sum is computed \n",
    "  only over those items, which are nonzero for both instances, i.e. only\n",
    "  films which are ranked by both persons are regarded.\n",
    "  If the parameter normed is True, then the euclidean distance is divided by\n",
    "  the number of non-zero elements integrated in the distance calculation. Thus\n",
    "  the effect of larger distances in the case of an increasing number of commonly ranked\n",
    "  items is avoided.\n",
    "  '''\n",
    "  # Get the list of shared_items\n",
    "  si={}\n",
    "  for item in prefs[person1]:\n",
    "    if item in prefs[person2]: si[item]=1\n",
    "  # len(si) counts the number of common ratings\n",
    "  # if they have no ratings in common, return 0\n",
    "  if len(si)==0: return 0\n",
    "\n",
    "  # Add up the squares of all the differences\n",
    "  sum_of_squares=np.sqrt(sum([pow(prefs[person1][item]-prefs[person2][item],2) \n",
    "                     for item in prefs[person1] if item in prefs[person2]]))\n",
    "  if normed:\n",
    "     sum_of_squares= 1.0/len(si)*sum_of_squares\n",
    "  return 1/(1+sum_of_squares)\n",
    "\n",
    "\n",
    "def sim_pearson(prefs,p1,p2):\n",
    "  '''\n",
    "  Returns the Pearson correlation coefficient for p1 and p2\n",
    "  '''\n",
    "    \n",
    "  # Get the list of commonly rated items\n",
    "  si={}\n",
    "  for item in prefs[p1]: \n",
    "    if item in prefs[p2]: si[item]=1\n",
    "\n",
    "  # if they are no ratings in common, return 0\n",
    "  if len(si)==0: return 0\n",
    "\n",
    "  # Sum calculations\n",
    "  n=len(si)\n",
    "  \n",
    "  # Calculate means of person 1 and 2\n",
    "  mp1=np.mean([prefs[p1][it] for it in si])\n",
    "  mp2=np.mean([prefs[p2][it] for it in si])\n",
    "  \n",
    "  # Calculate standard deviation of person 1 and 2\n",
    "  sp1=np.std([prefs[p1][it] for it in si])\n",
    "  sp2=np.std([prefs[p2][it] for it in si])\n",
    "  \n",
    "  # If all elements in one sample are identical, the standard deviation is 0. \n",
    "  # In this case there is no linear correlation between the samples\n",
    "  if sp1==0 or sp2==0:\n",
    "      return 0\n",
    "  r=1/(n*sp1*sp2)*sum([(prefs[p1][it]-mp1)*(prefs[p2][it]-mp2) for it in si])\n",
    "  return r\n",
    "\n",
    "\n",
    "def sim_RusselRao(prefs,person1,person2,normed=True):\n",
    "  ''' Returns RusselRao similaritiy between 2 users. The RusselRao similarity just counts the number\n",
    "  of common non-zero components of the two vectors and divides this number by N, where N is the length\n",
    "  of the vectors. If normed=False, the division by N is omitted.\n",
    "  '''\n",
    "  # Get the list of shared_items\n",
    "  si={}\n",
    "  commons=0\n",
    "  for item in prefs[person1]: \n",
    "    if prefs[person1][item]==1 and prefs[person2][item]==1:   \n",
    "        commons+=1\n",
    "  #print commons\n",
    "  if not normed:\n",
    "      return commons\n",
    "  else:\n",
    "      return commons*1.0/len(prefs[person1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:**\n",
    "1. Geben Sie die euklidische Ähnlichkeit und die Pearson Ähnlichkeit zwischen den Personen _Toby_ und _Lisa Rose_ aus.\n",
    "2. Diskutieren Sie die unterschiedlichen Ähnlichkeitswert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euklidische Aehnlichkeit:  0.615911621788925\n",
      "Pearson Aehnlichkeit:  0.9912407071619302\n"
     ]
    }
   ],
   "source": [
    "# 1. Aufgabe\n",
    "\n",
    "print('Euklidische Aehnlichkeit: ', sim_euclid(critics,'Lisa Rose','Toby'))\n",
    "print('Pearson Aehnlichkeit: ', sim_pearson(critics,'Lisa Rose','Toby'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ähnlichkeitswerte liefern für die User Lisa Rose und Toby ein sehr unterschiedliches Ergebnis. Lisa Rose hat insgesamt 6 Filme bewertet, während Toby nur 3 bewertet hat. Im Falle der euklidischen Ähnlichkeit sind nur die  absoluten Werte in die Berechnung eingeflossen, bei denen von beiden Personen Bewertungen vorliegen. Es ergibt sich daher ein Wert von 0.61.\n",
    "Die Pearson Ähnlichkeit mit 0.99 ist hier sehr hoch, da beide Personen die Filme ähnlich bewertet haben. Zum Beispiel ist der Film \"You, me and dupree\" im Vergleich zu den anderen Filmen von beiden am schlechtesten bewertet worden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aufgabe:__\n",
    "1. Schreiben Sie eine Funktion _topMatches(prefs,person,similarity)_, welche für eine beliebige in _critics_ enthaltene Person die Ähnlichkeitswerte zu allen anderen Personen berechnet und in einer geordneten Liste zurück gibt. Der Funktion soll als Übergabeparameter auch die anzuwendende Ähnlichkeitsfunktion (*sim_euclid* oder *sim_pearson*) übergeben werden können. Berechnen Sie mit dieser Funktion für jede Person die *top matches*, zunächst unter Verwendung der euklidischen- dann unter Verwendung der Pearson-Ähnlichkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Aufgabe\n",
    "\n",
    "def topMatches(prefs,person,similarity):\n",
    "\n",
    "    top_matches = []\n",
    "    \n",
    "    if similarity == 'sim_euclid':\n",
    "        for p in prefs:\n",
    "            if p != person:\n",
    "                top_matches.append([sim_euclid(prefs,person,p),p])    \n",
    "            top_matches.sort(reverse=True)\n",
    "        return top_matches\n",
    "  \n",
    "    elif similarity == 'sim_pearson':\n",
    "        for p in prefs:\n",
    "            if p != person:\n",
    "                top_matches.append([sim_pearson(prefs,person,p),p])   \n",
    "            top_matches.sort(reverse=True)\n",
    "        return top_matches\n",
    "\n",
    "    else:\n",
    "        return \"The given similarity was not correct use 'sim_euclid' or 'sim_pearson'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top matches of all persons\n",
    "\n",
    "top_matches_euclid = []\n",
    "top_matches_pearson = []\n",
    "\n",
    "for name in critics:\n",
    "    top_matches_euclid.append([name,topMatches(critics, name, 'sim_euclid')])\n",
    "    top_matches_pearson.append([name,topMatches(critics, name, 'sim_pearson')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Geben Sie mit der implementierten Funktion die *top matches* der Person Toby aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6666666666666666, 'Mick LaSalle'], [0.6246387977050463, 'Claudia Puig'], [0.615911621788925, 'Lisa Rose'], [0.5584815598877471, 'Michael Phillips'], [0.5227744249483389, 'Jack Matthews'], [0.5108747069239427, 'Gene Seymour']]\n",
      "[[0.9912407071619302, 'Lisa Rose'], [0.9244734516419051, 'Mick LaSalle'], [0.8934051474415642, 'Claudia Puig'], [0.6628489803598703, 'Jack Matthews'], [0.3812464258315117, 'Gene Seymour'], [-1.0, 'Michael Phillips']]\n"
     ]
    }
   ],
   "source": [
    "# 1. Aufgabe\n",
    "\n",
    "print(topMatches(critics, 'Toby', 'sim_euclid'))\n",
    "print(topMatches(critics, 'Toby', 'sim_pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Vergleichen Sie die beiden Ähnlichkeitsmaße. Welches Ähnlichkeitsmaß erscheint Ihnen für diesen Anwendungsfall sinnvoller und warum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Fall von Toby ist die pearson Ähnlichkeit die bessere Wahl. Kriterium ist hierbei ob beide Personen die gleichen Filme bereits bewertet haben und wie ähnlich sich die angegebenen Bewertungen sind. Dabei ist Toby am ähnlichsten mit der Person Lisa Rose. Beide haben die selben Filme bewertet und sehr ähnliche Bewertungen abgegeben. Am unähnlichsten ist Toby mit der Person Michael Philipps, was Sinn ergibt, da sie nur für zwei gleiche Filme eine Bewertung abgegeben haben.\n",
    "\n",
    "Für den Anwendungsfall der Filmempfehlung ist die Pearson Ähnlichkeit sinnvoller, da es Korrelationen aufzeigt, unabhängig von den absoluten Bewertungen. Jeder Mensch bewertet unterschiedlich, manche vergeben z.B. viel häufiger die maximale Punktzahl als andere, die hat bei der Pearson Ähnlichkeit keinen Einfluss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung von Empfehlungen mit User basiertem Collaborative Filtering\n",
    "Für die Produkte, die von einer Person noch nicht gekauft wurden, sollen Empfehlungen berechnet werden. Die Empfehlungen können ebenfalls Werte zwischen 1 (wird nicht empfohlen) und 5 (wird stark empfohlen) annehmen. Für die Berechnung der Empfehlung werden die Bewertungen des jeweiligen Produkts durch die anderen Personen herangezogen. Dabei werden die Bewertungen der ähnlichen Personen (d.h. hoher Pearson-Korrelationswert) stärker mit einbezogen als die Bewertungen durch Personen mit einem niedrigen Korrelationswert.\n",
    "\n",
    "__Beispiel:__\n",
    "Toby hat die Filme _The Night Listener_, _Lady in the Water_ und _Just My Luck_ noch nicht gekauft. Für diese Filme soll für Toby eine Empfehlung berechnet werden.\n",
    "In der unten aufgeführten Tabelle enthält die zweite Spalte die _Pearson-Ähnlichkeitswerte_ zwischen Toby und den anderen Personen. Die Spalten 3, 5 und 7 enthalten die Bewertungen der Filme _The Night Listener_, _Lady in the Water_ und _Just My Luck_ durch die anderen Personen. Die Spalten 4, 6 und 8 enthalten die jeweilige Filmbewertung gewichtet (mulipliziert) mit den Ähnlichkeitswerten der jeweiligen Person. Es fällt auf, dass in der Tabelle _Michael_ nicht enthalten ist. Das liegt daran, dass _Michael_ und _Toby_ einen negativen Ähnlichkeitswert aufweisen, d.h. deren Interessen sind gegenläufig. Personen mit negativem Ähnlichkeitswert sollten für Empfehlungen nicht berücksichtigt werden.\n",
    "Die Zeile _Sum_ enthält die Summe aller gewichteten Bewertungen. Aus diesem Wert allein kann die Empfehlung noch nicht abgeleitet werden, da Filme die nur von wenigen Personen bewertet wurden, eine relativ kleine Summe ergeben. Deshalb sollte _Sum_ noch durch die Anzahl der Bewertungen für diesen Film geteilt werden. Oder besser: Nicht durch die Summe der Bewertungen, sondern durch die Summe der relevanten Ähnlichkeitswerte (_KSum_). Der resultierende Empfehlungswert ist in der letzten Zeile eingetragen.\n",
    "\n",
    "\n",
    "![Abbildung Calculate Recommendation](https://www.hdm-stuttgart.de/~maucher/ipnotebooks/DataMining/Bilder/recommenderFilmCalculation.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aufgabe:__\n",
    "Schreiben Sie eine Funktion _getRecommendations(prefs,person,similarity)_, mit der die Empfehlungswerte berechnet werden können und bestimmen Sie die Empfehlungswerte für Toby. Der Funktion wird  \n",
    "\n",
    "* das Dictionary _critics_ mit den Filmbewertungen, \n",
    "* der Name der Person, für welche Empfehlungen berechnet werden sollen\n",
    "* die Methode für die Berechnung der Ähnlichkeit *sim_euclid* oder *sim_pearson*\n",
    "\n",
    "übergeben. Die Methode soll eine geordnete Liste zurück geben. Jedes Listenelement enthält an erster Stelle den berechneten Empfehlungswert und an zweiter Stelle den Namen des Films. Die Liste soll nach Empfehlungswerten absteigend geordnet sein.\n",
    "\n",
    "Testen Sie diese Funktion indem Sie die Empfehlungen für _Toby_ berechnen und mit den Werten in der oben aufgeführten Tabelle vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topMatches(prefs, person, similarity):\n",
    "    matches = []\n",
    "    for p in prefs:\n",
    "        if p != person:\n",
    "            matches.append((p, similarity(prefs, person, p)))\n",
    "            \n",
    "    matches.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisa Rose', 0.9912407071619302),\n",
       " ('Mick LaSalle', 0.9244734516419051),\n",
       " ('Claudia Puig', 0.8934051474415642),\n",
       " ('Jack Matthews', 0.6628489803598703),\n",
       " ('Gene Seymour', 0.3812464258315117),\n",
       " ('Michael Phillips', -1.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topMatches(critics, \"Toby\", sim_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vergleich\n",
    "Beim Vergleichen der Werte fällt auf, dass die Werte fast übereinstimmen. Wir gehen davon aus, dass die geringe Abweichung auf den gerundeten Werten der Tabelle basieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.3477895267131013, 'The Night Listener'),\n",
       " (2.8325499182641622, 'Lady in the Water'),\n",
       " (2.5309807037655645, 'Just My Luck')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getRecommendations(prefs, person, similarity):\n",
    "    recommendations = []\n",
    "    korrelation = topMatches(prefs,person,similarity)\n",
    "    #Alle Filme in eine Liste schreiben\n",
    "    filmList = []\n",
    "    for i in prefs:\n",
    "        for f in prefs[i]:\n",
    "            if f not in filmList:\n",
    "                filmList.append(f)\n",
    "    #schon bewertete Filme entfernen\n",
    "    for k,v in prefs[person].items():\n",
    "        if v>0:\n",
    "            filmList.remove(k)\n",
    "    \n",
    "    for film in filmList:\n",
    "        sumRating = 0\n",
    "        sumKorr = 0\n",
    "        \n",
    "        for person in korrelation:\n",
    "            if person[1] >=0:\n",
    "                #print(person)\n",
    "                if film in list(prefs[person[0]]):\n",
    "                    sumRating += person[1]*prefs[person[0]][film]\n",
    "                    sumKorr += person[1]\n",
    "        \n",
    "        recommendations.append((sumRating/sumKorr,film))\n",
    "    recommendations.sort(reverse=True)\n",
    "    return recommendations\n",
    "getRecommendations(critics,\"Toby\",sim_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung von Empfehlungen mit Item basiertem Collaborative Filtering\n",
    "In den vorigen Aufgaben wurden Ähnlichkeiten zwischen Personen bestimmt und für Produktempfehlungen benutzt (User basiertes Collaborative Filtering). Jetzt soll die Ähnlichkeit zwischen Produkten berechnet werden und auf der Basis dieser Produktähnlichkeit Empfehlungen berechnet werden (Item basiertes Collaborative Filtering).\n",
    "\n",
    "Dabei sollen die bereits implementierten Ähnlichkeitsfunktion *sim_euclid* und *sim_pearson* sowie die Ähnlichkeeits-Sortierfunktion *topMatches* unverändert eingesetzt werden.\n",
    "\n",
    "__Aufgabe:__\n",
    "\n",
    "1. Implementieren Sie eine Funktion, welche das Bewertungsdictionary *critics* derart transformiert, dass die Funktionen *sim_euclid*, *sim_pearson* und *topMatches* für das Item-basierte CF unverändert eingesetzt werden können. Die transformierte Matrix soll unter dem Namen *transCritics* abgespeichert werden.\n",
    "2. Schreiben Sie eine Funktion *calculateSimilarItems*, die aus der transformierten Matrix *transCritics* ein Dictionary berechnet, welches die Ähnlichkeit zwischen allen Filmen beschreibt. Die Keys des Dictionary sind die Filmnamen. Die Values sind geordnete Listen, welche die Funktion *topMatches* zurückgibt, wenn sie für die Filme (nicht für die User) aufgerufen wird. Dieses Dictionary wird an das aufrufende Programm zurück geben. \n",
    "3. Schreiben Sie eine Funktion *getRecommendedItems*, welche basierend auf dem im unten aufgeführten Beispiel dargestellten Verfahren unter Vorgabe der Bewertungsmatrix und der zu verwendenden Ähnlichkeitsfunktion Produktempfehlungen berechnet.\n",
    "4. Testen Sie die Funktion indem Sie die Empfehlungen für Toby berechnen und mit den Werten in der unten aufgeführten Tabelle vergleichen\n",
    "\n",
    "__Erläuterndes Beispiel:__\n",
    "\n",
    "_Toby_ hat die Filme *The Night Listener*, *Lady in the Water* und *Just My Luck* noch nicht gekauft. Für diese Filme soll für *Toby* eine Empfehlung berechnet werden. Gekauft und bewertet hat *Toby* die Filme *Snakes on a plane*, *Superman Returns* und *You and me and Dupree*. Diese bereits vorhandenen Filme bilden die erste Spalte der unten dargestellten Matrix. In der zweiten Spalte befinden sich _Toby's_ Bewertungen dieser Filme. Die Spalten 3,5 und 7 enthalten die Ähnlichkeitswerte (mit *calculateSimilarItems* unter Verwendung des normierten euklidischen Ähnlichkeitsmaßes berechnet) zwischen den drei von *Toby* noch nicht gekauften Filmen und den drei von _Toby_ bewerteten Filmen. Diese Ähnlichkeitswerte werden jeweils mit _Toby's_ Bewertungen multipliziert. Das Resultat dieser Multiplikation befindet sich in den Spalten 4,6 und 8. Der finale Empfehlungswert für die von _Toby_ noch nicht gekauften Filme wird berechnet in dem in den Spalten 4,6 und 8 zunächst die Summe über die Werte dieser Spalte in den drei oberen Zeilen berechnet wird und durch die Summe über die Werte der Spalten 3,5 und 7 geteilt wird. Im Fall, dass die *Pearson-Korrelation* zwischen den Filmen als Ähnlichkeitswert herangezogen wird, können negative Ähnlichkeitswerte auftreten. Dann soll in die Berechnung eines Empfehlungswert für Film A nur dann die Bewertung von Film B einfließen, wenn der Korrelationswert zwischen beiden $>0$ ist.  \n",
    "\n",
    "![Abbildung Calculate Itembased Recommendation](https://www.hdm-stuttgart.de/~maucher/ipnotebooks/DataMining/Bilder/recommenderFilmItemBased.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implementieren Sie eine Funktion, welche das Bewertungsdictionary *critics* derart transformiert, dass die Funktionen *sim_euclid*, *sim_pearson* und *topMatches* für das Item-basierte CF unverändert eingesetzt werden können. Die transformierte Matrix soll unter dem Namen *transCritics* abgespeichert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation des Dictionary\n",
    "\n",
    "Da wir in dieser Sektion das Item basierte CF einsetzen, wird das ursprüngliche Dictionary _critics_ transformiert, sodass anstelle des Nutzers als Key ein Produkt/Item verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aufgabe\n",
    "\n",
    "def transform(dictionary):\n",
    "    transDict = collections.defaultdict(dict)\n",
    "    for user, itemDict in dictionary.items():\n",
    "        for i in itemDict:\n",
    "            transDict[i][user] = itemDict[i] \n",
    "    return dict(transDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transCritics = transform(critics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Schreiben Sie eine Funktion *calculateSimilarItems*, die aus der transformierten Matrix *transCritics* ein Dictionary berechnet, welches die Ähnlichkeit zwischen allen Filmen beschreibt. Die Keys des Dictionary sind die Filmnamen. Die Values sind geordnete Listen, welche die Funktion *topMatches* zurückgibt, wenn sie für die Filme (nicht für die User) aufgerufen wird. Dieses Dictionary wird an das aufrufende Programm zurück geben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Aufgabe\n",
    "\n",
    "def calculateSimilarItems(transCritics, similarity):\n",
    "    similarDict = {}\n",
    "    for movie in transCritics:\n",
    "        similarDict[movie] = topMatches(transCritics, movie, similarity)\n",
    "    return similarDict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Schreiben Sie eine Funktion *getRecommendedItems*, welche basierend auf dem im unten aufgeführten Beispiel dargestellten Verfahren unter Vorgabe der Bewertungsmatrix und der zu verwendenden Ähnlichkeitsfunktion Produktempfehlungen berechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aufgabe\n",
    "\n",
    "def getRecommendedItems(prefs, person, similarity):\n",
    "    recommendations = []\n",
    "    filmKorrelation = calculateSimilarItems(transCritics, similarity)\n",
    "    #Alle Filme in eine Liste schreiben\n",
    "    filmList = []\n",
    "    for i in prefs:\n",
    "        for f in prefs[i]:\n",
    "            if f not in filmList:\n",
    "                filmList.append(f)\n",
    "    #schon bewertete Filme entfernen\n",
    "    filmList = set(filmList) - set(prefs[person])    \n",
    "    \n",
    "    for film in filmList:\n",
    "        sumKorr = 0\n",
    "        sumRatingKorr = 0.0\n",
    "        for ratedFilm in prefs[person]:\n",
    "            rating = prefs[person][ratedFilm]\n",
    "            for korrelation in filmKorrelation[ratedFilm]:\n",
    "                if korrelation[0] == film and korrelation[1] > 0:\n",
    "                    sumRatingKorr += rating * korrelation[1]\n",
    "                    sumKorr += korrelation[1]            \n",
    "           \n",
    "        if sumKorr == 0:\n",
    "            recommendations.append((0, film))\n",
    "        else:    \n",
    "            recommendations.append((sumRatingKorr/sumKorr, film))\n",
    "        \n",
    "    recommendations.sort(reverse=True)    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Testen Sie die Funktion indem Sie die Empfehlungen für Toby berechnen und mit den Werten in der unten aufgeführten Tabelle vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.20449096016088, 'The Night Listener'),\n",
       " (3.082136961799338, 'Lady in the Water'),\n",
       " (3.041861869079099, 'Just My Luck')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRecommendedItems(critics, \"Toby\", sim_euclid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Vergleichen der Werte fällt auf, dass die Werte fast übereinstimmen. Man kann daher davon ausgehen, dass die geringe Abweichung auf den gerundeten Werten der Tabelle basieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last.fm Musikempfehlungen\n",
    "Kopieren Sie die Datei _pylast.py_ vom _Resources_-Ordner im _DataMining_-Ordner des Skripteservers in das Verzeichnis dieses _IPython Notebooks_. In dieser Datei sind alle Zugriffsfunktionen auf _last.fm_ Dienste implementiert. Die notwendigen Anmelde- und Authentifizierungsdaten für den User _pythonlab_ sind ebenfalls schon in diesem Modul eingetragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pylast in c:\\users\\janina\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Collecting matplot\n",
      "  Using cached matplot-0.1.9-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting pyloco>=0.0.134\n",
      "  Using cached pyloco-0.0.139-py2.py3-none-any.whl (60 kB)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplot) (3.1.1)\n",
      "Processing c:\\users\\janina\\appdata\\local\\pip\\cache\\wheels\\9f\\d7\\59\\3026f39bd579bf6b18560faa960fd38957f31fe5ed845433e3\\simplewebsocketserver-0.1.1-cp37-none-any.whl\n",
      "Processing c:\\users\\janina\\appdata\\local\\pip\\cache\\wheels\\b8\\f2\\ed\\ea55f33eb524ba8afcfd55e93a6facc96ff23d637e4e609f6d\\ushlex-0.99.1-cp37-none-any.whl\n",
      "Collecting twine\n",
      "  Using cached twine-3.1.1-py3-none-any.whl (36 kB)\n",
      "Collecting websocket-client\n",
      "  Using cached websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting typing\n",
      "  Using cached typing-3.7.4.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->matplot) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->matplot) (1.18.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->matplot) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->matplot) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from matplotlib>=3.1.1->matplot) (2.8.1)\n",
      "Collecting readme-renderer>=21.0\n",
      "  Using cached readme_renderer-26.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm>=4.14 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from twine->pyloco>=0.0.134->matplot) (4.45.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from twine->pyloco>=0.0.134->matplot) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from twine->pyloco>=0.0.134->matplot) (46.1.3.post20200330)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\janina\\anaconda3\\lib\\site-packages (from twine->pyloco>=0.0.134->matplot) (1.5.0)\n",
      "Collecting requests-toolbelt!=0.9.0,>=0.8.0\n",
      "  Using cached requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: pkginfo>=1.4.2 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from twine->pyloco>=0.0.134->matplot) (1.5.0.1)\n",
      "Collecting keyring>=15.1\n",
      "  Downloading keyring-21.2.1-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six in c:\\users\\janina\\appdata\\roaming\\python\\python37\\site-packages (from websocket-client->pyloco>=0.0.134->matplot) (1.13.0)\n",
      "Requirement already satisfied: bleach>=2.1.0 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from readme-renderer>=21.0->twine->pyloco>=0.0.134->matplot) (3.1.4)\n",
      "Collecting docutils>=0.13.1\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Requirement already satisfied: Pygments>=2.5.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from readme-renderer>=21.0->twine->pyloco>=0.0.134->matplot) (2.6.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (2020.4.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\janina\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->twine->pyloco>=0.0.134->matplot) (3.1.0)\n",
      "Collecting pywin32-ctypes!=0.1.0,!=0.1.1; sys_platform == \"win32\"\n",
      "  Using cached pywin32_ctypes-0.2.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: webencodings in c:\\users\\janina\\anaconda3\\lib\\site-packages (from bleach>=2.1.0->readme-renderer>=21.0->twine->pyloco>=0.0.134->matplot) (0.5.1)\n",
      "Installing collected packages: SimpleWebSocketServer, ushlex, docutils, readme-renderer, requests-toolbelt, pywin32-ctypes, keyring, twine, websocket-client, typing, pyloco, matplot\n",
      "Successfully installed SimpleWebSocketServer-0.1.1 docutils-0.16 keyring-21.2.1 matplot-0.1.9 pyloco-0.0.139 pywin32-ctypes-0.2.0 readme-renderer-26.0 requests-toolbelt-0.9.1 twine-3.1.1 typing-3.7.4.1 ushlex-0.99.1 websocket-client-0.57.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pylast\n",
    "!pip install matplot\n",
    "!pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import pylast\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe:\n",
    "\n",
    "1. Stellen Sie durch Aufruf der Funktion *network=pylast.get_lastfm_network()* eine Verbindung zu *last.fm* her. Beim Aufruf der Funktion wird die Anmeldung und Authentifizierung durchgeführt. Die Funktion gibt ein Objekt der Klasse *Network* zurück. Über dieses Objekt können Methoden, wie\n",
    "\n",
    "    * *get_artist(\"kuenstlerName\")* (liefert Objekt der Klasse _Artist_)\n",
    "    * *get_album(\"albumName\")* (liefert Objekt der Klasse _Album_)\n",
    "    * *get_track(\"songName\")* (liefert Objekt der Klasse _Track_)\n",
    "    * *get_user(\"userName\"):* (liefert Objekt der Klass_Tag_)\n",
    "    * usw.\n",
    "    \n",
    "      aufgerufen werden. Die Menge aller verfügbaren Klassen und deren Attribute und Methoden können dem Modul _pylast.py_ entnommen werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key and secret key - All keys are stored in a private '.env' environment variable file.\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "API_SECRET = os.environ.get(\"API_SECRET\")\n",
    "# Authentication order to perform a write operation \n",
    "username = os.environ.get(\"username\")\n",
    "password_hash = pylast.md5(os.environ.get(\"password_hash\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aufgabe\n",
    "\n",
    "# Connect to last.fm API\n",
    "network = pylast.LastFMNetwork(api_key=API_KEY, api_secret=API_SECRET, username=username, password_hash=password_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Über das Pylast Framework wird die Verbindung zur lastFM API hergestellt. Ebenso wird hier auch das Errorhandling durch die pylast library übernommen. Verschiedene angebotene Methoden ermöglichen das Abrufen von Daten der lastFM API endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Rufen Sie über das oben instanziierte _Network_-Objekt die Methode *get_artist(\"BandIhrerWahl\")* auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Aufgabe\n",
    "\n",
    "# Keyword to search for\n",
    "band = \"Slipknot\"\n",
    "# Retrieve artist object from network\n",
    "artist = network.get_artist(band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Rufen Sie über das oben instanziierte _Artist_-Objekt die Methode *topfans=get_top_fans(10)* auf. Die Methode gibt eine Liste von _User_-Objekt/Gewichtung-Paaren zurück. Die Gewichtungen von Objekten werden in diesem Versuch nicht benötigt. Legen Sie deshalb mit _group=[a.item for a in topfan]_ eine Liste an, die nur noch die User Objekte enthält. **Wichtige Anmerkung:** Seit August 2015 gibt es Probleme mit der lastFM API Methode *get_top_fans()* (siehe auch: [pylast issues](https://github.com/pylast/pylast/issues/155s)). Falls am Versuchstermin der Fehler noch nicht behoben ist, können Sie den unten stehenden Code benutzen. Darin wird versucht auf die API-Methode zuzugreifen. Falls das nicht möglich ist, wird eine vordefinierte Liste von Usern angewandt. Diese Liste repräsentiert die *Top Fans* der Band *Slipknot* im Frühjahr 2015. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGroup(usernames):\n",
    "    group=[]\n",
    "    # Generate group of users\n",
    "    for user in usernames:\n",
    "        u = network.get_user(user)\n",
    "        group.append(u)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aufgabe\n",
    "# Retrieve data of top fans - hint the top_fans endpoint got removed at the last.fm API\n",
    "# In this case a dummy list with users from Slipknot in 2015 are used\n",
    "\n",
    "usernames=['BrunoJoS','DPREBOYE','MPistol40','NemoNightfall','SkyRif','Wags1382','Znapsen','cortapsyco','emill_67','sattuviitana']\n",
    "# Generate list of real user objects\n",
    "group = createGroup(usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementieren Sie eine Funktion *createLastfmUserDict()*. Dieser Funktion soll, die oben angelegte Liste von _User_-Objekten _group_ übergeben werden. Für jeden User in *group* sollen die 20 beliebtesten Bands mit der Methode *topartists=get_top_artists()[0:20]* bestimmt werden. Die Methode gibt eine Liste von *Artist*-Objekt/Gewichtung-Paaren zurück. Die Gewichtungen von Objekten werden in diesem Versuch nicht benötigt. Auf das *i.te* *Artist*-Objekt selbst kann mit *topartists[i].item* zugegriffen werden. Die Menge aller Bands, die auf diese Weise gesammelt werden, wird im folgenden mit _AllBands_ bezeichnet. D.h. in *AllBands* befinden sich alle Bands, die für mindestens einen User in *group* zu den Top-20 gehören. Nun soll ein verschachteltes Dictionary mit Namen *userDict* wie folgt angelegt werden:\n",
    "\n",
    "    * Die Keys sind die Namen der _User_-Objekte in _group_. Auf den Namen eines Objekts kann mit *get_name()* zugegriffen werden.\n",
    "    * Die Values sind selbst wieder Dictionaries, deren Keys die Namen der Bands in *AllBands* sind. Achten Sie auch hier darauf, dass Sie nicht das *Artist*-Objekt selbst, sondern dessen Namen als Key verwenden. \n",
    "    * Für den User *a* und die Band *b* ist der Value *userDict[a][b]= 1*, falls *b* zu den Top-20 des Users *a* gehört. Andernfalls ist *userDict[a][b]= 0*. \n",
    "    \n",
    "    Das derart angelegte Dictionary soll von der Funktion zurückgegeben werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Aufgabe\n",
    "\n",
    "\"\"\" \n",
    "The function createLastfmUserDict creates a UserDict dictionary with all users\n",
    "and their top twenty artists with additional values of 0 or 1 to rate if a user has selected the\n",
    "band before to their favorites or not.\n",
    "\"\"\"\n",
    "def createLastfmUserDict(usergroup):\n",
    "    userDict = {}\n",
    "    allbands = []\n",
    "    \n",
    "    for user in usergroup:\n",
    "        bands = []\n",
    "        # api call to fetch twenty most pop artists\n",
    "        topartists = user.get_top_artists()[:20]                \n",
    "        # filter all unique bands\n",
    "        for i in range(len(topartists)):\n",
    "            if topartists[i] in allbands:\n",
    "                pass\n",
    "            else:\n",
    "                allbands.append(topartists[i].item)\n",
    "                bands.append(topartists[i].item.get_name())\n",
    "        # store user values in nested dict\n",
    "        userDict[user.get_name()] = bands\n",
    "    \n",
    "    # fill user dictionary\n",
    "    for user, bands in userDict.items():\n",
    "        banddict = {}\n",
    "        # iterate through all bands\n",
    "        for i in range(len(allbands)):\n",
    "            # check if bandname fits\n",
    "            if allbands[i].get_name() in bands:\n",
    "                banddict[allbands[i].get_name()] = 1\n",
    "            else:\n",
    "                banddict[allbands[i].get_name()] = 0                \n",
    "        userDict[user] = banddict\n",
    "    return userDict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Wählen Sie jetzt einen beliebigen User aus *group*. Bestimmen Sie zu diesem User die ähnlichsten User in *group* durch Anwendung der im ersten Teilversuch implementierten Funktion *topMatches()*. Der Funktion wird das angelegte *userDict* und der Name des gewählten Users übergeben. Als Ähnlichkeitsmaß soll die euklidische Metrik angewandt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "WSError",
     "evalue": "Invalid API key - You must be granted a valid key by last.fm",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b2088da48329>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# picked the user 'NemoNightfall'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NemoNightfall'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0muserDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateLastfmUserDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# print(userDict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c23ce0bc5153>\u001b[0m in \u001b[0;36mcreateLastfmUserDict\u001b[1;34m(usergroup)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mbands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# api call to fetch twenty most pop artists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtopartists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_top_artists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# filter all unique bands\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopartists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pylast\\__init__.py\u001b[0m in \u001b[0;36mget_top_artists\u001b[1;34m(self, period, limit)\u001b[0m\n\u001b[0;32m   2493\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"limit\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2495\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mws_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".getTopArtists\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2497\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_extract_top_artists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pylast\\__init__.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method_name, cacheable, params)\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_Request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcacheable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pylast\\__init__.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, cacheable)\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_cached_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mminidom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"opensearch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pylast\\__init__.py\u001b[0m in \u001b[0;36m_download_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_response_for_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pylast\\__init__.py\u001b[0m in \u001b[0;36m_check_response_for_errors\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAttribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mdetails\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirstChild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWSError\u001b[0m: Invalid API key - You must be granted a valid key by last.fm"
     ]
    }
   ],
   "source": [
    "# 5. Aufgabe\n",
    "\n",
    "# picked the user 'NemoNightfall'\n",
    "user = 'NemoNightfall'\n",
    "userDict = createLastfmUserDict(group)\n",
    "# print(userDict)\n",
    "\n",
    "# Calculate the most similar users to 'user'\n",
    "similarityList = topMatches(userDict, user, sim_euclid)\n",
    "print('Most similar users to {} are: \\n{}'.format(user, similarityList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Bestimmen Sie dann für den gewählten User Band-Empfehlungen durch Anwendung der im ersten Teilversuch implementierten Funktion *getRecommendations()*. Der Funktion wird das angelegte *userDict* und der Name des gewählten Users übergeben. Als Ähnlichkeitsmaß soll die euklidische Metrik, danach die Russel_Rao Metrik, angewandt werden.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-38beb59c5999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Song recommendations for the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msongList1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRecommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_euclid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msongList2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetRecommendations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserDict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_RusselRao\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'userDict' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. Aufgabe\n",
    "\n",
    "# Song recommendations for the user\n",
    "songList1 = getRecommendations(userDict, user, sim_euclid)\n",
    "songList2 = getRecommendations(userDict, user, sim_RusselRao)\n",
    "\n",
    "pp.pprint('Recommendet songs for {} are: \\n{}'.format(user, songList1))\n",
    "pp.pprint('Recommendet songs for {} are: \\n{}'.format(user, songList2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Diskutieren Sie das Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-854240b3fdd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdifList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mband1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msongList1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mband2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msongList2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "difList = []\n",
    "for band1 in songList1:\n",
    "    for band2 in songList2:\n",
    "        if band1[1] == band2[1]:\n",
    "            scoreDif = band1[0] - band2[0]\n",
    "            difList.append((scoreDif, band1[1]))\n",
    "difList.sort(reverse = True)\n",
    "\n",
    "difDf = pd.DataFrame(difList)\n",
    "difDf['colors'] = ['red' if x < 0 else 'green' for x in difDf[0]]\n",
    "difDf\n",
    "plt.figure(figsize = (14, 30), dpi = 80)\n",
    "plt.hlines(y=difDf[1], xmin=0, xmax=difDf[0], color=difDf.colors, linewidth=5)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.gca().set(ylabel=\"$Bands$\", xlabel=\"$Difference$\")\n",
    "plt.title(\"Difference between between Euclid and Russel Rao\", fontdict={'size':20})\n",
    "plt.grid(linestyle=\"-\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 8}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songList1 = sorted(songList1[:10])\n",
    "songList2 = sorted(songList2[:10])\n",
    "\n",
    "x_val = [format(song[0]*100, '.2f') for song in songList1]\n",
    "y_val = [song[1] for song in songList1]\n",
    "x_val1 = [format(song[0]*100, '.2f') for song in songList2]\n",
    "y_val1 = [song[1] for song in songList2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 6), sharey=True)\n",
    "axs[0].bar(y_val, x_val)\n",
    "axs[1].bar(y_val1, x_val1)\n",
    "axs[0].set_yticks(np.arange(0, 10.0, step=10.0))\n",
    "axs[1].set_yticks(np.arange(0, 10.0, step=10.0))\n",
    "fig.suptitle('Band Recommendations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diskussion der Ergebnisse\n",
    "\n",
    "Bei der Untersuchung der ersten zehn Band-Empfehlungen mit dem höchsten Wahrscheinlichkeitswert für deren Ähnlichkeit zu von anderen Personen favorisierten Bands, konnte unter der Hinzunahme der _euklidischen Distanz_ , als auch der _Russel Rao_ Metrik ein sehr ähnliches Ergebnis erzielt werden. In beiden Ergebnisslisten finden sich die Bands _Korn_ (euclid/56%, russel_rao/59.9%), _Rage Against the Machine_  (euclid/44%, russel_rao/43.9%), als auch _Rammstein_  (euclid/34%, russel_rao/43.9%) unter den ersten drei Empfehlungen. Abnehmend von hier unterschiedet sich das Ergebnis leicht im berechneten Wert, inhaltlich sind die ersten zehn Bands jedoch namentlich in beiden Listen vertreten. \n",
    "\n",
    "Wahrscheinlich ist aufgrund der Empfehlungswerte, dass der beispielhaft gewählte Nutzer \"NemoNightfall\" eine Tendenz zu alternativen Musikrichtungen, wie Metal, als auch Rock hat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zusatzaufgabe LightFM Library\n",
    "\n",
    "This is an experiment with our testuser 'NemoNightFall' of the Last.fm database. The idea is to fetch his favorite bands and train the LightFM algorithm based on previously rated songs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightfm\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_movielens(min_rating=5.0)\n",
    "print(repr(data['train']))\n",
    "#testuser = network.get_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training via sgd (stochastic gradient descent), \n",
    "# for every epoch the model learns to fit the data more and more closely.\n",
    "model = LightFM(loss='warp')\n",
    "%time model.fit(data['train'], epochs=30, num_threads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model \n",
    "print(\"Train precision: %.2f\" % precision_at_k(model, data['train'], k=5).mean())\n",
    "print(\"Test precision: %.2f\" % precision_at_k(model, data['test'], k=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recommendation(model, data, user_ids):\n",
    "    \n",
    "\n",
    "    n_users, n_items = data['train'].shape\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        \n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        top_items = data['item_labels'][np.argsort(-scores)]\n",
    "        \n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\"     Known positives:\")\n",
    "        \n",
    "        for x in known_positives[:3]:\n",
    "            print(\"        %s\" % x)\n",
    "\n",
    "        print(\"     Recommended:\")\n",
    "        \n",
    "        for x in top_items[:3]:\n",
    "            print(\"        %s\" % x)\n",
    "        \n",
    "sample_recommendation(model, data, [3, 25, 450])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda0567885b7d8c440f82fd92f12f60d2f6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "664px",
   "left": "0px",
   "right": "1209.67px",
   "top": "125.333px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
